{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjhdC0JSrC8Rcyp/htx9D9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MirudulaShri260302/LLM_Data/blob/main/Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0zs7JfFFRRD"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "print(f\"Number of lines in dataset: {len(dataset)}\")\n"
      ],
      "metadata": {
        "id": "7PbeLGQRMbaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<PARA>\"]})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "para_id = tokenizer.convert_tokens_to_ids(\"<PARA>\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EQ82po_eMciU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n"
      ],
      "metadata": {
        "id": "D4qkpPccMd_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"First 20 input IDs:\", tokenized_ds[0][\"input_ids\"][:20])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vywbez9EMgPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated_inputs = []\n",
        "    concatenated_masks = []\n",
        "\n",
        "    for ids, mask in zip(examples[\"input_ids\"], examples[\"attention_mask\"]):\n",
        "        concatenated_inputs.extend(ids)\n",
        "        concatenated_inputs.append(para_id)  # INSERT <PARA> TOKEN HERE\n",
        "        concatenated_masks.extend(mask)\n",
        "        concatenated_masks.append(1)\n",
        "\n",
        "    total_len = (len(concatenated_inputs) // block_size) * block_size\n",
        "\n",
        "    concatenated_inputs = concatenated_inputs[:total_len]\n",
        "    concatenated_masks = concatenated_masks[:total_len]\n",
        "\n",
        "    result_input_ids = [concatenated_inputs[i:i+block_size]\n",
        "                        for i in range(0, total_len, block_size)]\n",
        "    result_masks = [concatenated_masks[i:i+block_size]\n",
        "                    for i in range(0, total_len, block_size)]\n",
        "\n",
        "    return {\"input_ids\": result_input_ids, \"attention_mask\": result_masks}\n"
      ],
      "metadata": {
        "id": "H_pTtd1TMicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_ds = tokenized_ds.map(group_texts, batched=True, batch_size=1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "CMWaTIV1MkrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
        "\n",
        "train_loader = DataLoader(lm_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "V4SgvrZ-Mnr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(batch[\"input_ids\"].shape)\n",
        "\n",
        "    decoded = tokenizer.decode(batch[\"input_ids\"][0], skip_special_tokens=False)\n",
        "    print(decoded[:300])\n",
        "    break\n"
      ],
      "metadata": {
        "id": "OCOG3S1sOGB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HReelpVlOH10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}