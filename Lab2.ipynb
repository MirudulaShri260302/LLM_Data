{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MirudulaShri260302/LLM_Data/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fed0c33",
      "metadata": {
        "id": "2fed0c33"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Streaming Language Modeling Data Pipeline with Hugging Face Datasets\n",
        "--------------------------------------------------------------------\n",
        "Goal:\n",
        "    Demonstrate how to build a *true streaming* LM pipeline that:\n",
        "    - Processes data without loading the entire dataset into RAM.\n",
        "    - Tokenizes on the fly.\n",
        "    - Concatenates text and chunks into fixed-length blocks for LM training.\n",
        "    - Produces batches ready for training in PyTorch.\n",
        "\n",
        "Key Teaching Points:\n",
        "    1. Streaming allows us to work with web-scale corpora.\n",
        "    2. We still can do grouping/chunking in a rolling fashion.\n",
        "    3. This approach mimics real-world pipelines for large-scale LM training.\n",
        "\"\"\"\n",
        "print(__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae182f4",
      "metadata": {
        "id": "0ae182f4"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers, AutoTokenizer, torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190f9fe4",
      "metadata": {
        "id": "190f9fe4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04603b35",
      "metadata": {
        "id": "04603b35"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1. Load the dataset in STREAMING mode\n",
        "# ============================================================\n",
        "stream_dataset = load_dataset(\n",
        "    \"wikitext\",\n",
        "    \"wikitext-2-raw-v1\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36818657",
      "metadata": {
        "id": "36818657"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2. Initialize GPT-2 tokenizer\n",
        "# ============================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token   # GPT-2 normally has no pad token\n",
        "\n",
        "eos_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ee19e3",
      "metadata": {
        "id": "09ee19e3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. Tokenization step (no padding, no truncation)\n",
        "# ============================================================\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized_stream = stream_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4779cf2",
      "metadata": {
        "id": "d4779cf2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. Rolling buffer with EOS insertion + padding final block\n",
        "# ============================================================\n",
        "block_size = 128\n",
        "\n",
        "def group_texts_streaming(dataset_iter, block_size, eos_id):\n",
        "    \"\"\"\n",
        "    Modified version:\n",
        "      ✔ Adds EOS token between documents\n",
        "      ✔ Pads final leftover block instead of dropping it\n",
        "    \"\"\"\n",
        "    buffer = []\n",
        "\n",
        "    for example in dataset_iter:\n",
        "        # Add tokens for this document\n",
        "        buffer.extend(example[\"input_ids\"])\n",
        "\n",
        "        # Insert EOS token between documents\n",
        "        buffer.append(eos_id)\n",
        "\n",
        "        # Produce all full blocks\n",
        "        while len(buffer) >= block_size:\n",
        "            chunk = buffer[:block_size]\n",
        "            buffer = buffer[block_size:]\n",
        "            yield {\n",
        "                \"input_ids\": chunk,\n",
        "                \"attention_mask\": [1] * block_size\n",
        "            }\n",
        "\n",
        "    # After dataset is exhausted — pad leftover tokens\n",
        "    if len(buffer) > 0:\n",
        "        pad_length = block_size - len(buffer)\n",
        "        padded_chunk = buffer + [eos_id] * pad_length\n",
        "\n",
        "        attn_mask = [1] * len(buffer) + [0] * pad_length\n",
        "\n",
        "        yield {\n",
        "            \"input_ids\": padded_chunk,\n",
        "            \"attention_mask\": attn_mask\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d766de",
      "metadata": {
        "id": "51d766de"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. IterableDataset wrapper\n",
        "# ============================================================\n",
        "class StreamingLMIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_iterable_dataset, block_size):\n",
        "        self.dataset = hf_iterable_dataset\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return group_texts_streaming(self.dataset, self.block_size, eos_id)\n",
        "\n",
        "\n",
        "grouped_iterable_dataset = StreamingLMIterableDataset(tokenized_stream, block_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b9f8fe",
      "metadata": {
        "id": "60b9f8fe"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. Collate function\n",
        "# ============================================================\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in batch], dtype=torch.long)\n",
        "\n",
        "    # For LM training, labels = input_ids\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": input_ids.clone()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43f40b7",
      "metadata": {
        "id": "b43f40b7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. DataLoader for streaming data\n",
        "# ============================================================\n",
        "train_loader = DataLoader(\n",
        "    grouped_iterable_dataset,\n",
        "    batch_size=8,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. Iterate over a few batches\n",
        "# ============================================================\n",
        "print(\"Sample streaming batches (with EOS + padded final block):\")\n",
        "for i, batch in enumerate(train_loader):\n",
        "    print(f\"Batch {i} -> input_ids shape: {batch['input_ids'].shape}\")\n",
        "    if i == 2:\n",
        "        break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample streaming batches (with EOS + padded final block):\")\n",
        "for i, batch in enumerate(train_loader):\n",
        "    print(f\"\\nBatch {i} -> input_ids shape: {batch['input_ids'].shape}\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # PRINT SENTENCES (new code)\n",
        "    # ----------------------------------------\n",
        "    # Print the first sequence in the batch\n",
        "    seq = batch[\"input_ids\"][0].tolist()\n",
        "    decoded = tokenizer.decode(seq, skip_special_tokens=False)\n",
        "\n",
        "    print(\"Decoded text sample:\")\n",
        "    print(decoded)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    if i == 2:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "gjVfXF2m8t8r"
      },
      "id": "gjVfXF2m8t8r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}